{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_LSTM_multi_label_binary**6_combined_training_6_classes",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEuuGIOACiV_",
        "outputId": "06ddd232-82bd-41d5-c135-29de2ec50d1d"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpNEdSpUh9Kq",
        "outputId": "e0f7bdd6-5f55-475d-a97e-4b83495affe1"
      },
      "source": [
        "############### upload data\n",
        "# Mount Google drive to load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDgdevlejEHj",
        "outputId": "2df62e96-cf84-4652-e3db-ca01280f567f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "path ='/content/drive/MyDrive/ece367/ECE324 project/dataProcessing/processed_data_aug/test.csv'\n",
        "\n",
        "text_data = pd.read_csv(path, sep=',')\n",
        "print(text_data)\n",
        "print(text_data[\"toxic\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    id  ... identity_hate\n",
            "0     2a2805dc384e9b60  ...             1\n",
            "1     cc738bd89c1a1dc5  ...             0\n",
            "2     218bdcbae72b1def  ...             0\n",
            "3     d3bf5e38c76a5c0d  ...             0\n",
            "4     586a2aefd01e5316  ...             0\n",
            "...                ...  ...           ...\n",
            "2534  2489283ccba99a24  ...             0\n",
            "2535  5e70615ce678ee05  ...             0\n",
            "2536  5e0584dcf993c0aa  ...             0\n",
            "2537  35243af9766437b5  ...             1\n",
            "2538  71c6a5551bc1b17e  ...             0\n",
            "\n",
            "[2539 rows x 8 columns]\n",
            "1    2063\n",
            "0     476\n",
            "Name: toxic, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEkARCFCUV8z"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import datetime, time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkvzWrU_7vbr"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab, hidden_dim=100):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 100,kernel_size=(2,embedding_dim)) #in_channels, out_chanels, kernel_size\n",
        "        \n",
        "        \n",
        "        self.lstm1 = nn.LSTM(embedding_dim,hidden_dim)\n",
        "        target_size = 1\n",
        "        self.fc1 = nn.Linear(hidden_dim, target_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "    #   embeds = self.word_embeddings(sentence)\n",
        "    #     lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "    #     tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "    #     tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "    #     return tag_scores\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        # print('x.shape',x.shape)\n",
        "\n",
        "        x = x.unsqueeze(0)\n",
        "        # print('x = x.unsqueeze(0)',x.shape)\n",
        "        x = x.transpose(1,2) # swaps 2nd and 3rd dimension\n",
        "        # print('x = x.transpose(1,2)',x.shape)\n",
        "        x = x.transpose(0,1) # swaps 1st and 2nd dimension, now it has the correct input (n_samples,channels, height, width)\n",
        "        # print('x = x.transpose(0,1)',x.shape)\n",
        "        x1 = F.relu(self.conv1(x)) \n",
        "        # print('x1 = F.relu(self.conv1(x))',x1.shape)\n",
        "        \n",
        "        x1 = x1.squeeze(3)\n",
        "        # print('x1 = x1.squeeze(3)',x1.shape)\n",
        "        pool1 = nn.MaxPool1d(x1.size(2), 1) #maxpool\n",
        "        \n",
        "        x = pool1(x1)\n",
        "        # print('pool1 = nn.MaxPool1d(x1.size(2), 1)',x.shape)\n",
        "        x = x.squeeze(2)\n",
        "        x = x.unsqueeze(0)\n",
        "        # print('x = x.squeeze(2)',x.shape)\n",
        "\n",
        "\n",
        "        #  pack the word embeddings in the batch together and \n",
        "        # run the RNN on this object\n",
        "        # x = pack_padded_sequence(x,lengths) \n",
        "        # print('x.shape',x.shape)\n",
        "        x,(h,c) = self.lstm1(x)\n",
        "        # print('x.shape',x.shape)\n",
        "        # print('h.shape',h.shape)\n",
        "        # print('len(h)',len(h))\n",
        "        # print('(x.view(len(lengths),-1)).shape',(x.view(len(lengths),-1)).shape)\n",
        "        h_result = self.fc1(h.squeeze(0))\n",
        "        # h_result = F.log_softmax(tag_space, dim=1) --> change to this with multiple cat\n",
        "        h_result = torch.sigmoid(h_result)\n",
        "        \n",
        "\n",
        "        return h_result.squeeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQrS1SRWI66w"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ix1PMK_mmd9o",
        "outputId": "e3876850-0f4d-42ce-cd1d-91a90f2e5fba"
      },
      "source": [
        "trainRNN_R(seed = 0, batch_size = 128, learning_rate = 0.001, epochs = 30, model = CNN_LSTM, emb_dim = 100, rnn_hidden_dim = 100, num_filt = 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:26, 2.23MB/s]                          \n",
            "100%|█████████▉| 399239/400000 [00:17<00:00, 22505.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch---------------------------------------------- 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399239/400000 [00:30<00:00, 22505.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[1] toxic_train loss: 0.475, toxic_train acc: 0.838, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.528, toxic_testing acc: 0.786 \n",
            "[1] severe_toxic_train loss: 0.702, severe_toxic_train acc: 0.781, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[1] obscene_train loss: 0.595, obscene_train acc: 0.737, obscene_validation loss: 0.502, obscene_validation acc: 0.800,        obscene_testing loss: 0.526, obscene_testing acc: 0.905 \n",
            "[1] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[1] insult_train loss: 0.622, insult_train acc: 0.689, insult_validation loss: 0.411, insult_validation acc: 0.950,        insult_testing loss: 0.543, insult_testing acc: 0.857 \n",
            "[1] identity_hate_train loss: 0.702, identity_hate_train acc: 0.785, identity_hate_validation loss: 0.693, identity_hate_validation acc: 0.725,        identity_hate_testing loss: 0.693, identity_hate_testing acc: 0.762 \n",
            "[1] comb_train loss: 0.671, comb_train acc: 0.212, comb_validation loss: 0.604, comb_validation acc: 0.236,        comb_testing loss: 0.689, comb_testing acc: 0.221  \n",
            "epoch---------------------------------------------- 1\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[2] toxic_train loss: 0.470, toxic_train acc: 0.843, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.528, toxic_testing acc: 0.786 \n",
            "[2] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[2] obscene_train loss: 0.559, obscene_train acc: 0.829, obscene_validation loss: 0.501, obscene_validation acc: 0.825,        obscene_testing loss: 0.526, obscene_testing acc: 0.881 \n",
            "[2] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[2] insult_train loss: 0.589, insult_train acc: 0.779, insult_validation loss: 0.425, insult_validation acc: 0.925,        insult_testing loss: 0.548, insult_testing acc: 0.833 \n",
            "[2] identity_hate_train loss: 0.693, identity_hate_train acc: 0.787, identity_hate_validation loss: 0.693, identity_hate_validation acc: 0.725,        identity_hate_testing loss: 0.693, identity_hate_testing acc: 0.762 \n",
            "[2] comb_train loss: 0.668, comb_train acc: 0.236, comb_validation loss: 0.604, comb_validation acc: 0.242,        comb_testing loss: 0.689, comb_testing acc: 0.243  \n",
            "epoch---------------------------------------------- 2\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[3] toxic_train loss: 0.470, toxic_train acc: 0.843, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.528, toxic_testing acc: 0.786 \n",
            "[3] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[3] obscene_train loss: 0.553, obscene_train acc: 0.842, obscene_validation loss: 0.505, obscene_validation acc: 0.800,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[3] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[3] insult_train loss: 0.581, insult_train acc: 0.798, insult_validation loss: 0.410, insult_validation acc: 0.950,        insult_testing loss: 0.546, insult_testing acc: 0.857 \n",
            "[3] identity_hate_train loss: 0.693, identity_hate_train acc: 0.787, identity_hate_validation loss: 0.693, identity_hate_validation acc: 0.725,        identity_hate_testing loss: 0.693, identity_hate_testing acc: 0.762 \n",
            "[3] comb_train loss: 0.668, comb_train acc: 0.243, comb_validation loss: 0.604, comb_validation acc: 0.241,        comb_testing loss: 0.689, comb_testing acc: 0.232  \n",
            "epoch---------------------------------------------- 3\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[4] toxic_train loss: 0.470, toxic_train acc: 0.843, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.528, toxic_testing acc: 0.786 \n",
            "[4] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[4] obscene_train loss: 0.550, obscene_train acc: 0.849, obscene_validation loss: 0.500, obscene_validation acc: 0.825,        obscene_testing loss: 0.522, obscene_testing acc: 0.905 \n",
            "[4] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[4] insult_train loss: 0.574, insult_train acc: 0.814, insult_validation loss: 0.409, insult_validation acc: 0.950,        insult_testing loss: 0.555, insult_testing acc: 0.810 \n",
            "[4] identity_hate_train loss: 0.693, identity_hate_train acc: 0.787, identity_hate_validation loss: 0.693, identity_hate_validation acc: 0.725,        identity_hate_testing loss: 0.693, identity_hate_testing acc: 0.762 \n",
            "[4] comb_train loss: 0.668, comb_train acc: 0.247, comb_validation loss: 0.604, comb_validation acc: 0.240,        comb_testing loss: 0.689, comb_testing acc: 0.229  \n",
            "epoch---------------------------------------------- 4\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[5] toxic_train loss: 0.451, toxic_train acc: 0.862, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.501, toxic_testing acc: 0.833 \n",
            "[5] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[5] obscene_train loss: 0.546, obscene_train acc: 0.859, obscene_validation loss: 0.507, obscene_validation acc: 0.800,        obscene_testing loss: 0.518, obscene_testing acc: 0.905 \n",
            "[5] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[5] insult_train loss: 0.571, insult_train acc: 0.822, insult_validation loss: 0.412, insult_validation acc: 0.950,        insult_testing loss: 0.558, insult_testing acc: 0.810 \n",
            "[5] identity_hate_train loss: 0.693, identity_hate_train acc: 0.787, identity_hate_validation loss: 0.693, identity_hate_validation acc: 0.725,        identity_hate_testing loss: 0.693, identity_hate_testing acc: 0.762 \n",
            "[5] comb_train loss: 0.659, comb_train acc: 0.282, comb_validation loss: 0.604, comb_validation acc: 0.310,        comb_testing loss: 0.676, comb_testing acc: 0.292  \n",
            "epoch---------------------------------------------- 5\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[6] toxic_train loss: 0.424, toxic_train acc: 0.906, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.444, toxic_testing acc: 0.905 \n",
            "[6] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[6] obscene_train loss: 0.544, obscene_train acc: 0.865, obscene_validation loss: 0.499, obscene_validation acc: 0.825,        obscene_testing loss: 0.527, obscene_testing acc: 0.881 \n",
            "[6] threat_train loss: 0.866, threat_train acc: 0.795, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[6] insult_train loss: 0.565, insult_train acc: 0.833, insult_validation loss: 0.418, insult_validation acc: 0.925,        insult_testing loss: 0.559, insult_testing acc: 0.810 \n",
            "[6] identity_hate_train loss: 0.693, identity_hate_train acc: 0.787, identity_hate_validation loss: 0.693, identity_hate_validation acc: 0.725,        identity_hate_testing loss: 0.693, identity_hate_testing acc: 0.762 \n",
            "[6] comb_train loss: 0.645, comb_train acc: 0.326, comb_validation loss: 0.604, comb_validation acc: 0.304,        comb_testing loss: 0.647, comb_testing acc: 0.303  \n",
            "epoch---------------------------------------------- 6\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[7] toxic_train loss: 0.417, toxic_train acc: 0.919, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.451, toxic_testing acc: 0.929 \n",
            "[7] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[7] obscene_train loss: 0.539, obscene_train acc: 0.874, obscene_validation loss: 0.503, obscene_validation acc: 0.800,        obscene_testing loss: 0.515, obscene_testing acc: 0.905 \n",
            "[7] threat_train loss: 0.866, threat_train acc: 0.795, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[7] insult_train loss: 0.561, insult_train acc: 0.842, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.558, insult_testing acc: 0.810 \n",
            "[7] identity_hate_train loss: 0.693, identity_hate_train acc: 0.787, identity_hate_validation loss: 0.693, identity_hate_validation acc: 0.725,        identity_hate_testing loss: 0.693, identity_hate_testing acc: 0.762 \n",
            "[7] comb_train loss: 0.642, comb_train acc: 0.340, comb_validation loss: 0.604, comb_validation acc: 0.310,        comb_testing loss: 0.651, comb_testing acc: 0.307  \n",
            "epoch---------------------------------------------- 7\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[8] toxic_train loss: 0.414, toxic_train acc: 0.925, toxic_validation loss: 0.389, toxic_validation acc: 0.925,         toxic_testing loss: 0.444, toxic_testing acc: 0.905 \n",
            "[8] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[8] obscene_train loss: 0.538, obscene_train acc: 0.876, obscene_validation loss: 0.507, obscene_validation acc: 0.800,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[8] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[8] insult_train loss: 0.558, insult_train acc: 0.850, insult_validation loss: 0.406, insult_validation acc: 0.975,        insult_testing loss: 0.558, insult_testing acc: 0.810 \n",
            "[8] identity_hate_train loss: 0.693, identity_hate_train acc: 0.787, identity_hate_validation loss: 0.693, identity_hate_validation acc: 0.725,        identity_hate_testing loss: 0.693, identity_hate_testing acc: 0.762 \n",
            "[8] comb_train loss: 0.640, comb_train acc: 0.347, comb_validation loss: 0.605, comb_validation acc: 0.302,        comb_testing loss: 0.648, comb_testing acc: 0.298  \n",
            "epoch---------------------------------------------- 8\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[9] toxic_train loss: 0.412, toxic_train acc: 0.928, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.443, toxic_testing acc: 0.929 \n",
            "[9] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[9] obscene_train loss: 0.535, obscene_train acc: 0.883, obscene_validation loss: 0.509, obscene_validation acc: 0.800,        obscene_testing loss: 0.531, obscene_testing acc: 0.881 \n",
            "[9] threat_train loss: 0.866, threat_train acc: 0.795, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[9] insult_train loss: 0.556, insult_train acc: 0.855, insult_validation loss: 0.414, insult_validation acc: 0.950,        insult_testing loss: 0.555, insult_testing acc: 0.810 \n",
            "[9] identity_hate_train loss: 0.693, identity_hate_train acc: 0.787, identity_hate_validation loss: 0.693, identity_hate_validation acc: 0.725,        identity_hate_testing loss: 0.693, identity_hate_testing acc: 0.762 \n",
            "[9] comb_train loss: 0.639, comb_train acc: 0.352, comb_validation loss: 0.604, comb_validation acc: 0.297,        comb_testing loss: 0.647, comb_testing acc: 0.307  \n",
            "epoch---------------------------------------------- 9\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[10] toxic_train loss: 0.410, toxic_train acc: 0.934, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.442, toxic_testing acc: 0.929 \n",
            "[10] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[10] obscene_train loss: 0.533, obscene_train acc: 0.888, obscene_validation loss: 0.508, obscene_validation acc: 0.800,        obscene_testing loss: 0.525, obscene_testing acc: 0.881 \n",
            "[10] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[10] insult_train loss: 0.553, insult_train acc: 0.860, insult_validation loss: 0.408, insult_validation acc: 0.975,        insult_testing loss: 0.546, insult_testing acc: 0.833 \n",
            "[10] identity_hate_train loss: 0.693, identity_hate_train acc: 0.787, identity_hate_validation loss: 0.693, identity_hate_validation acc: 0.725,        identity_hate_testing loss: 0.694, identity_hate_testing acc: 0.762 \n",
            "[10] comb_train loss: 0.638, comb_train acc: 0.361, comb_validation loss: 0.604, comb_validation acc: 0.305,        comb_testing loss: 0.647, comb_testing acc: 0.309  \n",
            "epoch---------------------------------------------- 10\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[11] toxic_train loss: 0.407, toxic_train acc: 0.938, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.440, toxic_testing acc: 0.929 \n",
            "[11] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[11] obscene_train loss: 0.531, obscene_train acc: 0.892, obscene_validation loss: 0.507, obscene_validation acc: 0.800,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[11] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[11] insult_train loss: 0.549, insult_train acc: 0.869, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.549, insult_testing acc: 0.810 \n",
            "[11] identity_hate_train loss: 0.693, identity_hate_train acc: 0.788, identity_hate_validation loss: 0.674, identity_hate_validation acc: 0.800,        identity_hate_testing loss: 0.710, identity_hate_testing acc: 0.738 \n",
            "[11] comb_train loss: 0.637, comb_train acc: 0.368, comb_validation loss: 0.604, comb_validation acc: 0.307,        comb_testing loss: 0.645, comb_testing acc: 0.303  \n",
            "epoch---------------------------------------------- 11\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[12] toxic_train loss: 0.406, toxic_train acc: 0.941, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.448, toxic_testing acc: 0.905 \n",
            "[12] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[12] obscene_train loss: 0.529, obscene_train acc: 0.897, obscene_validation loss: 0.511, obscene_validation acc: 0.800,        obscene_testing loss: 0.528, obscene_testing acc: 0.881 \n",
            "[12] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[12] insult_train loss: 0.547, insult_train acc: 0.875, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.547, insult_testing acc: 0.833 \n",
            "[12] identity_hate_train loss: 0.692, identity_hate_train acc: 0.790, identity_hate_validation loss: 0.693, identity_hate_validation acc: 0.725,        identity_hate_testing loss: 0.693, identity_hate_testing acc: 0.762 \n",
            "[12] comb_train loss: 0.636, comb_train acc: 0.374, comb_validation loss: 0.604, comb_validation acc: 0.307,        comb_testing loss: 0.649, comb_testing acc: 0.312  \n",
            "epoch---------------------------------------------- 12\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[13] toxic_train loss: 0.405, toxic_train acc: 0.944, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.440, toxic_testing acc: 0.929 \n",
            "[13] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[13] obscene_train loss: 0.528, obscene_train acc: 0.900, obscene_validation loss: 0.493, obscene_validation acc: 0.825,        obscene_testing loss: 0.518, obscene_testing acc: 0.905 \n",
            "[13] threat_train loss: 0.866, threat_train acc: 0.795, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[13] insult_train loss: 0.546, insult_train acc: 0.876, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.547, insult_testing acc: 0.833 \n",
            "[13] identity_hate_train loss: 0.690, identity_hate_train acc: 0.797, identity_hate_validation loss: 0.636, identity_hate_validation acc: 0.875,        identity_hate_testing loss: 0.675, identity_hate_testing acc: 0.810 \n",
            "[13] comb_train loss: 0.635, comb_train acc: 0.381, comb_validation loss: 0.604, comb_validation acc: 0.308,        comb_testing loss: 0.645, comb_testing acc: 0.310  \n",
            "epoch---------------------------------------------- 13\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[14] toxic_train loss: 0.404, toxic_train acc: 0.945, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.440, toxic_testing acc: 0.929 \n",
            "[14] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[14] obscene_train loss: 0.528, obscene_train acc: 0.900, obscene_validation loss: 0.511, obscene_validation acc: 0.800,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[14] threat_train loss: 0.866, threat_train acc: 0.795, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[14] insult_train loss: 0.542, insult_train acc: 0.885, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.551, insult_testing acc: 0.833 \n",
            "[14] identity_hate_train loss: 0.684, identity_hate_train acc: 0.816, identity_hate_validation loss: 0.617, identity_hate_validation acc: 0.925,        identity_hate_testing loss: 0.640, identity_hate_testing acc: 0.905 \n",
            "[14] comb_train loss: 0.635, comb_train acc: 0.391, comb_validation loss: 0.604, comb_validation acc: 0.328,        comb_testing loss: 0.645, comb_testing acc: 0.335  \n",
            "epoch---------------------------------------------- 14\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[15] toxic_train loss: 0.403, toxic_train acc: 0.947, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.441, toxic_testing acc: 0.929 \n",
            "[15] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[15] obscene_train loss: 0.528, obscene_train acc: 0.899, obscene_validation loss: 0.508, obscene_validation acc: 0.800,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[15] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[15] insult_train loss: 0.543, insult_train acc: 0.884, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.554, insult_testing acc: 0.810 \n",
            "[15] identity_hate_train loss: 0.673, identity_hate_train acc: 0.851, identity_hate_validation loss: 0.617, identity_hate_validation acc: 0.925,        identity_hate_testing loss: 0.639, identity_hate_testing acc: 0.905 \n",
            "[15] comb_train loss: 0.635, comb_train acc: 0.404, comb_validation loss: 0.604, comb_validation acc: 0.332,        comb_testing loss: 0.646, comb_testing acc: 0.337  \n",
            "epoch---------------------------------------------- 15\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[16] toxic_train loss: 0.402, toxic_train acc: 0.948, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.440, toxic_testing acc: 0.929 \n",
            "[16] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.783, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.694, severe_toxic_testing acc: 0.357 \n",
            "[16] obscene_train loss: 0.526, obscene_train acc: 0.904, obscene_validation loss: 0.508, obscene_validation acc: 0.800,        obscene_testing loss: 0.521, obscene_testing acc: 0.905 \n",
            "[16] threat_train loss: 0.866, threat_train acc: 0.795, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[16] insult_train loss: 0.540, insult_train acc: 0.890, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.541, insult_testing acc: 0.857 \n",
            "[16] identity_hate_train loss: 0.666, identity_hate_train acc: 0.869, identity_hate_validation loss: 0.615, identity_hate_validation acc: 0.925,        identity_hate_testing loss: 0.639, identity_hate_testing acc: 0.905 \n",
            "[16] comb_train loss: 0.634, comb_train acc: 0.418, comb_validation loss: 0.604, comb_validation acc: 0.318,        comb_testing loss: 0.646, comb_testing acc: 0.328  \n",
            "epoch---------------------------------------------- 16\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[17] toxic_train loss: 0.404, toxic_train acc: 0.946, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.439, toxic_testing acc: 0.929 \n",
            "[17] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.694, severe_toxic_testing acc: 0.357 \n",
            "[17] obscene_train loss: 0.525, obscene_train acc: 0.905, obscene_validation loss: 0.494, obscene_validation acc: 0.825,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[17] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[17] insult_train loss: 0.540, insult_train acc: 0.890, insult_validation loss: 0.416, insult_validation acc: 0.950,        insult_testing loss: 0.544, insult_testing acc: 0.833 \n",
            "[17] identity_hate_train loss: 0.661, identity_hate_train acc: 0.882, identity_hate_validation loss: 0.617, identity_hate_validation acc: 0.925,        identity_hate_testing loss: 0.639, identity_hate_testing acc: 0.905 \n",
            "[17] comb_train loss: 0.635, comb_train acc: 0.421, comb_validation loss: 0.604, comb_validation acc: 0.323,        comb_testing loss: 0.645, comb_testing acc: 0.341  \n",
            "epoch---------------------------------------------- 17\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[18] toxic_train loss: 0.402, toxic_train acc: 0.950, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.445, toxic_testing acc: 0.905 \n",
            "[18] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.694, severe_toxic_testing acc: 0.357 \n",
            "[18] obscene_train loss: 0.526, obscene_train acc: 0.905, obscene_validation loss: 0.510, obscene_validation acc: 0.800,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[18] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[18] insult_train loss: 0.542, insult_train acc: 0.885, insult_validation loss: 0.406, insult_validation acc: 0.975,        insult_testing loss: 0.544, insult_testing acc: 0.833 \n",
            "[18] identity_hate_train loss: 0.657, identity_hate_train acc: 0.891, identity_hate_validation loss: 0.608, identity_hate_validation acc: 0.950,        identity_hate_testing loss: 0.637, identity_hate_testing acc: 0.905 \n",
            "[18] comb_train loss: 0.634, comb_train acc: 0.424, comb_validation loss: 0.604, comb_validation acc: 0.331,        comb_testing loss: 0.648, comb_testing acc: 0.334  \n",
            "epoch---------------------------------------------- 18\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[19] toxic_train loss: 0.401, toxic_train acc: 0.952, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.444, toxic_testing acc: 0.905 \n",
            "[19] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[19] obscene_train loss: 0.523, obscene_train acc: 0.911, obscene_validation loss: 0.493, obscene_validation acc: 0.825,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[19] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[19] insult_train loss: 0.540, insult_train acc: 0.889, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.549, insult_testing acc: 0.833 \n",
            "[19] identity_hate_train loss: 0.653, identity_hate_train acc: 0.901, identity_hate_validation loss: 0.608, identity_hate_validation acc: 0.950,        identity_hate_testing loss: 0.636, identity_hate_testing acc: 0.905 \n",
            "[19] comb_train loss: 0.633, comb_train acc: 0.435, comb_validation loss: 0.604, comb_validation acc: 0.327,        comb_testing loss: 0.648, comb_testing acc: 0.330  \n",
            "epoch---------------------------------------------- 19\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[20] toxic_train loss: 0.401, toxic_train acc: 0.952, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.439, toxic_testing acc: 0.929 \n",
            "[20] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[20] obscene_train loss: 0.524, obscene_train acc: 0.909, obscene_validation loss: 0.493, obscene_validation acc: 0.825,        obscene_testing loss: 0.528, obscene_testing acc: 0.881 \n",
            "[20] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[20] insult_train loss: 0.539, insult_train acc: 0.891, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.544, insult_testing acc: 0.833 \n",
            "[20] identity_hate_train loss: 0.651, identity_hate_train acc: 0.905, identity_hate_validation loss: 0.601, identity_hate_validation acc: 0.950,        identity_hate_testing loss: 0.627, identity_hate_testing acc: 0.952 \n",
            "[20] comb_train loss: 0.633, comb_train acc: 0.437, comb_validation loss: 0.604, comb_validation acc: 0.334,        comb_testing loss: 0.645, comb_testing acc: 0.343  \n",
            "epoch---------------------------------------------- 20\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[21] toxic_train loss: 0.400, toxic_train acc: 0.953, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.441, toxic_testing acc: 0.929 \n",
            "[21] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[21] obscene_train loss: 0.522, obscene_train acc: 0.912, obscene_validation loss: 0.508, obscene_validation acc: 0.800,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[21] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[21] insult_train loss: 0.536, insult_train acc: 0.899, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.544, insult_testing acc: 0.833 \n",
            "[21] identity_hate_train loss: 0.649, identity_hate_train acc: 0.911, identity_hate_validation loss: 0.607, identity_hate_validation acc: 0.950,        identity_hate_testing loss: 0.630, identity_hate_testing acc: 0.929 \n",
            "[21] comb_train loss: 0.633, comb_train acc: 0.447, comb_validation loss: 0.604, comb_validation acc: 0.339,        comb_testing loss: 0.646, comb_testing acc: 0.344  \n",
            "epoch---------------------------------------------- 21\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[22] toxic_train loss: 0.399, toxic_train acc: 0.954, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.439, toxic_testing acc: 0.929 \n",
            "[22] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[22] obscene_train loss: 0.521, obscene_train acc: 0.916, obscene_validation loss: 0.503, obscene_validation acc: 0.800,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[22] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[22] insult_train loss: 0.535, insult_train acc: 0.901, insult_validation loss: 0.406, insult_validation acc: 0.975,        insult_testing loss: 0.544, insult_testing acc: 0.833 \n",
            "[22] identity_hate_train loss: 0.647, identity_hate_train acc: 0.915, identity_hate_validation loss: 0.605, identity_hate_validation acc: 0.950,        identity_hate_testing loss: 0.630, identity_hate_testing acc: 0.929 \n",
            "[22] comb_train loss: 0.633, comb_train acc: 0.453, comb_validation loss: 0.604, comb_validation acc: 0.328,        comb_testing loss: 0.645, comb_testing acc: 0.340  \n",
            "epoch---------------------------------------------- 22\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[23] toxic_train loss: 0.400, toxic_train acc: 0.954, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.453, toxic_testing acc: 0.881 \n",
            "[23] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[23] obscene_train loss: 0.523, obscene_train acc: 0.911, obscene_validation loss: 0.506, obscene_validation acc: 0.800,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[23] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[23] insult_train loss: 0.538, insult_train acc: 0.894, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.545, insult_testing acc: 0.833 \n",
            "[23] identity_hate_train loss: 0.646, identity_hate_train acc: 0.918, identity_hate_validation loss: 0.590, identity_hate_validation acc: 1.000,        identity_hate_testing loss: 0.636, identity_hate_testing acc: 0.929 \n",
            "[23] comb_train loss: 0.633, comb_train acc: 0.446, comb_validation loss: 0.604, comb_validation acc: 0.335,        comb_testing loss: 0.652, comb_testing acc: 0.333  \n",
            "epoch---------------------------------------------- 23\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[24] toxic_train loss: 0.400, toxic_train acc: 0.953, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.439, toxic_testing acc: 0.929 \n",
            "[24] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.694, severe_toxic_testing acc: 0.357 \n",
            "[24] obscene_train loss: 0.522, obscene_train acc: 0.913, obscene_validation loss: 0.494, obscene_validation acc: 0.825,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[24] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[24] insult_train loss: 0.535, insult_train acc: 0.900, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.544, insult_testing acc: 0.833 \n",
            "[24] identity_hate_train loss: 0.644, identity_hate_train acc: 0.922, identity_hate_validation loss: 0.591, identity_hate_validation acc: 1.000,        identity_hate_testing loss: 0.633, identity_hate_testing acc: 0.905 \n",
            "[24] comb_train loss: 0.633, comb_train acc: 0.451, comb_validation loss: 0.604, comb_validation acc: 0.332,        comb_testing loss: 0.645, comb_testing acc: 0.346  \n",
            "epoch---------------------------------------------- 24\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[25] toxic_train loss: 0.401, toxic_train acc: 0.951, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.445, toxic_testing acc: 0.905 \n",
            "[25] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[25] obscene_train loss: 0.520, obscene_train acc: 0.917, obscene_validation loss: 0.499, obscene_validation acc: 0.825,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[25] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[25] insult_train loss: 0.534, insult_train acc: 0.903, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.548, insult_testing acc: 0.833 \n",
            "[25] identity_hate_train loss: 0.642, identity_hate_train acc: 0.927, identity_hate_validation loss: 0.604, identity_hate_validation acc: 0.950,        identity_hate_testing loss: 0.630, identity_hate_testing acc: 0.929 \n",
            "[25] comb_train loss: 0.634, comb_train acc: 0.457, comb_validation loss: 0.604, comb_validation acc: 0.327,        comb_testing loss: 0.648, comb_testing acc: 0.332  \n",
            "epoch---------------------------------------------- 25\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[26] toxic_train loss: 0.400, toxic_train acc: 0.952, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.439, toxic_testing acc: 0.929 \n",
            "[26] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[26] obscene_train loss: 0.519, obscene_train acc: 0.919, obscene_validation loss: 0.521, obscene_validation acc: 0.775,        obscene_testing loss: 0.529, obscene_testing acc: 0.881 \n",
            "[26] threat_train loss: 0.866, threat_train acc: 0.795, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[26] insult_train loss: 0.532, insult_train acc: 0.906, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.536, insult_testing acc: 0.857 \n",
            "[26] identity_hate_train loss: 0.642, identity_hate_train acc: 0.928, identity_hate_validation loss: 0.608, identity_hate_validation acc: 0.950,        identity_hate_testing loss: 0.630, identity_hate_testing acc: 0.929 \n",
            "[26] comb_train loss: 0.633, comb_train acc: 0.460, comb_validation loss: 0.604, comb_validation acc: 0.322,        comb_testing loss: 0.645, comb_testing acc: 0.335  \n",
            "epoch---------------------------------------------- 26\n",
            "cat ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "here\n",
            "///////// val total 1////////// 3240\n",
            "val_total 3240\n",
            "here\n",
            "///////// val total 1////////// 3242\n",
            "[27] toxic_train loss: 0.399, toxic_train acc: 0.955, toxic_validation loss: 0.388, toxic_validation acc: 0.925,         toxic_testing loss: 0.439, toxic_testing acc: 0.929 \n",
            "[27] severe_toxic_train loss: 0.693, severe_toxic_train acc: 0.784, severe_toxic_validation loss: 0.693, severe_toxic_validation acc: 0.325,        severe_toxic_testing loss: 0.693, severe_toxic_testing acc: 0.357 \n",
            "[27] obscene_train loss: 0.522, obscene_train acc: 0.913, obscene_validation loss: 0.493, obscene_validation acc: 0.825,        obscene_testing loss: 0.528, obscene_testing acc: 0.881 \n",
            "[27] threat_train loss: 0.866, threat_train acc: 0.796, threat_validation loss: 0.820, threat_validation acc: 0.700,        threat_testing loss: 0.851, threat_testing acc: 0.762 \n",
            "[27] insult_train loss: 0.533, insult_train acc: 0.905, insult_validation loss: 0.405, insult_validation acc: 0.975,        insult_testing loss: 0.545, insult_testing acc: 0.833 \n",
            "[27] identity_hate_train loss: 0.641, identity_hate_train acc: 0.930, identity_hate_validation loss: 0.614, identity_hate_validation acc: 0.950,        identity_hate_testing loss: 0.630, identity_hate_testing acc: 0.929 \n",
            "[27] comb_train loss: 0.633, comb_train acc: 0.458, comb_validation loss: 0.604, comb_validation acc: 0.333,        comb_testing loss: 0.645, comb_testing acc: 0.340  \n",
            "epoch---------------------------------------------- 27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2c1d1da11694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainRNN_R\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_LSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-f761318d33b1>\u001b[0m in \u001b[0;36mtrainRNN_R\u001b[0;34m(seed, batch_size, learning_rate, epochs, model, emb_dim, rnn_hidden_dim, num_filt)\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'toxic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'severe_toxic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'obscene'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'threat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'insult'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'identity_hate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6d99df893f0d>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(cat, vocab, train_iter, val_iter, test_iter, seed, learning_rate, epochs, model, emb_dim, rnn_hidden_dim, num_filt)\u001b[0m\n\u001b[1;32m    374\u001b[0m               \u001b[0midentity_hate_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midentity_hate_outputs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0midentity_hate_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m               \u001b[0mcomb_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mevaluate_combined_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoxic_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoxic_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msevere_toxic_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msevere_toxic_labels\u001b[0m\u001b[0;34m,\u001b[0m                                                        \u001b[0mobscene_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobscene_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreat_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreat_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minsult_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minsult_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midentity_hate_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midentity_hate_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m               \u001b[0;31m##### evaluate training accuracy ###############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6d99df893f0d>\u001b[0m in \u001b[0;36mevaluate_combined_correct\u001b[0;34m(toxic_outputs, toxic_labels, severe_toxic_outputs, severe_toxic_labels, obscene_outputs, obscene_labels, threat_outputs, threat_labels, insult_outputs, insult_labels, identity_hate_outputs, identity_hate_labels)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;31m# print(temp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mcomb_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# print('comb_correct',comb_correct)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jJdM4Axma5I"
      },
      "source": [
        "def trainRNN_R(seed = 0, batch_size = 64, learning_rate = 0.001, epochs = 25, model = CNN_LSTM, emb_dim = 100, rnn_hidden_dim = 100, num_filt = 50):\n",
        "    \n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  COMMENT_TEXT    = data.Field(sequential=True,lower=True, tokenize='spacy', include_lengths=True)\n",
        "  TOXIC           = data.Field(sequential=False, use_vocab=False)\n",
        "  SEVERE_TOXIC    = data.Field(sequential=False, use_vocab=False)\n",
        "  OBSCENE         = data.Field(sequential=False, use_vocab=False)\n",
        "  THREAT          = data.Field(sequential=False, use_vocab=False)\n",
        "  INSULT          = data.Field(sequential=False, use_vocab=False)\n",
        "  IDENTITY_HATE   = data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "  train_data, val_data, test_data = data.TabularDataset.splits(\n",
        "      \n",
        "          path='/content/drive/MyDrive/ece367/ECE324 project/dataProcessing/processed_data_aug_v2/', train='train.csv',\n",
        "          validation='valid.csv', test='test.csv', format='csv',\n",
        "          skip_header=True, fields=[('id', None), ('text', COMMENT_TEXT), ('toxic', TOXIC), ('severe_toxic', SEVERE_TOXIC), ('obscene', OBSCENE), ('threat', THREAT), ('insult', INSULT), ('identity_hate', IDENTITY_HATE)])\n",
        "          \n",
        "          # path='/content/drive/MyDrive/ece367/ECE324 project/dataProcessing/new_processed_data/', train='train.csv',\n",
        "          # validation='valid.csv', test='test.csv', format='csv',\n",
        "          # skip_header=True, fields=[('id', None), ('text', COMMENT_TEXT), ('toxic', TOXIC), ('severe_toxic', SEVERE_TOXIC), ('obscene', OBSCENE), ('threat', THREAT), ('insult', INSULT), ('identity_hate', IDENTITY_HATE)])\n",
        "          \n",
        "          \n",
        "          # path='/content/drive/MyDrive/ece367/ECE324 project/dataProcessing/processed_data_aug/', train='train.csv',\n",
        "          # validation='valid.csv', test='test.csv', format='csv',\n",
        "          # skip_header=True, fields=[('id', None), ('text', COMMENT_TEXT), ('toxic', TOXIC), ('severe_toxic', SEVERE_TOXIC), ('obscene', OBSCENE), ('threat', THREAT), ('insult', INSULT), ('identity_hate', IDENTITY_HATE)])\n",
        "          \n",
        "          # path='/content/drive/MyDrive/ece367/ECE324 project/dataProcessing/multi_loose_large_data/', train='train.csv',\n",
        "          # validation='valid.csv', test='test.csv', format='csv',\n",
        "          # skip_header=True, fields=[('id', None), ('text', COMMENT_TEXT), ('toxic', TOXIC), ('severe_toxic', SEVERE_TOXIC), ('obscene', OBSCENE), ('threat', THREAT), ('insult', INSULT), ('identity_hate', IDENTITY_HATE)])\n",
        "          \n",
        "  train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "      (train_data, val_data, test_data), batch_sizes=(batch_size, batch_size,batch_size),\n",
        "      sort_key=lambda x: len(x.text), device=None, sort_within_batch=True, repeat=False)\n",
        "\n",
        "  COMMENT_TEXT.build_vocab(train_data, val_data, test_data)\n",
        "\n",
        "  COMMENT_TEXT.vocab.load_vectors(torchtext.vocab.GloVe(name='6B', dim=100))\n",
        "  vocab = COMMENT_TEXT.vocab\n",
        "\n",
        "  def save_vocab(vocab):\n",
        "    import pickle\n",
        "    output = open('vocab.pkl', 'wb')\n",
        "    pickle.dump(vocab, output)\n",
        "    output.close()\n",
        "  save_vocab(vocab)\n",
        "\n",
        "  cat = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "\n",
        "  train_classifier(cat,vocab,train_iter, val_iter, test_iter,seed, learning_rate, epochs, model, emb_dim, rnn_hidden_dim, num_filt) \n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEs3DUk0_NxM"
      },
      "source": [
        "def batch_label(batch, cat):\n",
        "    if cat == 'toxic':\n",
        "      return batch.toxic.float() \n",
        "    elif cat == 'severe_toxic':\n",
        "      return batch.severe_toxic.float() \n",
        "    elif cat == 'obscene':\n",
        "      return batch.obscene.float() \n",
        "    elif cat == 'threat':\n",
        "      return batch.threat.float() \n",
        "    elif cat == 'insult':\n",
        "      return batch.insult.float() \n",
        "    elif cat == 'identity_hate':\n",
        "      return batch.identity_hate.float() \n",
        "    else:\n",
        "      print(\"ERROR: wrong category name\")\n",
        "\n",
        "def get_binary_accuracy(model,batch_input,batch_input_length,learning_rate,labels):  \n",
        "    valCorrect = 0\n",
        "    valRunningLoss = 0.0\n",
        "    valNumberOfBatches = 0\n",
        "    valTotal = 0\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    outputs = model(batch_input, batch_input_length)\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "    valRunningLoss += loss.item()\n",
        "    outputs = (outputs>0.5).float()\n",
        "    \n",
        "    valCorrect += (outputs == labels).sum().item()\n",
        "    valTotal += labels.size(0)\n",
        "    # print('valTotal += labels.size(0)',valTotal)\n",
        "    valNumberOfBatches += 1\n",
        "    valRunningLoss = valRunningLoss/valNumberOfBatches\n",
        "    valCorrect = valCorrect/valTotal\n",
        "\n",
        "    return valRunningLoss,valCorrect,outputs,valTotal\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_combined_correct(toxic_outputs,toxic_labels,severe_toxic_outputs,severe_toxic_labels,\\\n",
        "                                                        obscene_outputs,obscene_labels,threat_outputs,threat_labels,insult_outputs,insult_labels,identity_hate_outputs,identity_hate_labels):\n",
        "    comb_correct = 0\n",
        "    comb_outputs = torch.cat((toxic_outputs.unsqueeze(0), severe_toxic_outputs.unsqueeze(0),obscene_outputs.unsqueeze(0),\\\n",
        "                              threat_outputs.unsqueeze(0),insult_outputs.unsqueeze(0),identity_hate_outputs.unsqueeze(0)),0)\n",
        "    comb_outputs = torch.transpose(comb_outputs, 0, 1)\n",
        "    # print('##############')\n",
        "    # print('comb_outputs',comb_outputs)\n",
        "    # print('comb_outputs shape',comb_outputs.shape)\n",
        "\n",
        "    comb_labels = torch.cat((toxic_labels.unsqueeze(0), severe_toxic_labels.unsqueeze(0),obscene_labels.unsqueeze(0),\\\n",
        "                              threat_labels.unsqueeze(0),insult_labels.unsqueeze(0),identity_hate_labels.unsqueeze(0)),0)\n",
        "    comb_labels = torch.transpose(comb_labels, 0, 1)\n",
        "    # print('comb_labels',comb_labels)\n",
        "    # print('comb_labels shape',comb_labels.shape)\n",
        "    # print('##############')\n",
        "    temp = (comb_labels == comb_outputs)\n",
        "    for i in temp:\n",
        "      # print(temp)\n",
        "      if torch.equal(i, torch.BoolTensor([True, True,True, True,True, True])):\n",
        "        comb_correct += 1\n",
        "    # print('comb_correct',comb_correct)\n",
        "    # print('##############')\n",
        "    return comb_correct\n",
        "    \n",
        "def evaluate_binary_loss_correct(data_iter,toxic_model,severe_toxic_model,obscene_model,threat_model,insult_model,identity_hate_model,cat,learning_rate):\n",
        "    print('here')     \n",
        "\n",
        "    val_comb_correct = 0\n",
        "    valTotal = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in data_iter:\n",
        "        batch_input, batch_input_length = batch.text\n",
        "        \n",
        "        toxic_valLabels = batch_label(batch,cat[0])  \n",
        "        severe_toxic_valLabels = batch_label(batch,cat[1]) \n",
        "        obscene_valLabels = batch_label(batch,cat[2]) \n",
        "        threat_valLabels = batch_label(batch,cat[3]) \n",
        "        insult_valLabels = batch_label(batch,cat[4]) \n",
        "        identity_hate_valLabels = batch_label(batch,cat[5])          \n",
        "\n",
        "        toxic_valRunningLoss,toxic_valCorrect,toxic_valOutputs,val_batch_total = get_binary_accuracy(toxic_model,batch_input,batch_input_length,learning_rate,toxic_valLabels)\n",
        "        severe_toxic_valRunningLoss, severe_toxic_valCorrect, severe_toxic_valOutputs, val_batch_total = get_binary_accuracy( severe_toxic_model,batch_input,batch_input_length,learning_rate, severe_toxic_valLabels)\n",
        "        obscene_valRunningLoss, obscene_valCorrect, obscene_valOutputs, val_batch_total = get_binary_accuracy( obscene_model,batch_input,batch_input_length,learning_rate, obscene_valLabels)\n",
        "        threat_valRunningLoss,threat_valCorrect,threat_valOutputs,val_batch_total = get_binary_accuracy(threat_model,batch_input,batch_input_length,learning_rate,threat_valLabels)\n",
        "        \n",
        "        insult_valRunningLoss, insult_valCorrect, insult_valOutputs, val_batch_total = get_binary_accuracy( insult_model,batch_input,batch_input_length,learning_rate, insult_valLabels)\n",
        "        identity_hate_valRunningLoss, identity_hate_valCorrect, identity_hate_valOutputs, val_batch_total = get_binary_accuracy(identity_hate_model,batch_input,batch_input_length,learning_rate, identity_hate_valLabels)\n",
        "        \n",
        "        val_comb_correct += evaluate_combined_correct(toxic_valOutputs,toxic_valLabels,severe_toxic_valOutputs,severe_toxic_valLabels,obscene_valOutputs,obscene_valLabels,threat_valOutputs,threat_valLabels, insult_valOutputs,insult_valLabels,identity_hate_valOutputs,identity_hate_valLabels)\n",
        "        valTotal += val_batch_total\n",
        "    # print statistics for every epoch)\n",
        "    print('///////// val total 1//////////',valTotal)\n",
        "    return toxic_valRunningLoss,toxic_valCorrect,\\\n",
        "    severe_toxic_valRunningLoss, severe_toxic_valCorrect, \\\n",
        "    obscene_valRunningLoss, obscene_valCorrect, \\\n",
        "    threat_valRunningLoss,threat_valCorrect,\\\n",
        "    insult_valRunningLoss, insult_valCorrect,\\\n",
        "    identity_hate_valRunningLoss, identity_hate_valCorrect, \\\n",
        "    val_comb_correct,valTotal\n",
        "\n",
        "def train_classifier(cat ,vocab,train_iter, val_iter, test_iter,seed, learning_rate, epochs, model, emb_dim, rnn_hidden_dim, num_filt): \n",
        "    torch.manual_seed(seed)    \n",
        "    \n",
        "    epochCount = []  \n",
        "    \n",
        "\n",
        "    ######## toxic ###########\n",
        "    toxic_model = model(emb_dim, vocab)\n",
        "    toxic_criterion = nn.BCEWithLogitsLoss()\n",
        "    toxic_optimizer = optim.Adam(toxic_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    toxic_lossCount = []\n",
        "    toxic_accCount = []\n",
        "    \n",
        "    toxic_valLossCount = []\n",
        "    toxic_valAccCount = []    \n",
        "\n",
        "    toxic_testLossCount = []\n",
        "    toxic_testAccCount = []\n",
        "\n",
        "    ######## severe_toxic ###########\n",
        "    severe_toxic_model = model(emb_dim, vocab)\n",
        "    severe_toxic_criterion = nn.BCEWithLogitsLoss()\n",
        "    severe_toxic_optimizer = optim.Adam(severe_toxic_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    severe_toxic_valLossCount = []\n",
        "    severe_toxic_valAccCount = []\n",
        "    \n",
        "    severe_toxic_lossCount = []\n",
        "    severe_toxic_accCount = []\n",
        "\n",
        "    severe_toxic_testLossCount = []\n",
        "    severe_toxic_testAccCount = []\n",
        "\n",
        "    ######## obscene ###########\n",
        "    obscene_model = model(emb_dim, vocab)\n",
        "    obscene_criterion = nn.BCEWithLogitsLoss()\n",
        "    obscene_optimizer = optim.Adam(obscene_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    obscene_valLossCount = []\n",
        "    obscene_valAccCount = []\n",
        "    \n",
        "    obscene_lossCount = []\n",
        "    obscene_accCount = []\n",
        "\n",
        "    obscene_testLossCount = []\n",
        "    obscene_testAccCount = []\n",
        "\n",
        "    ######## threat ###########\n",
        "    threat_model = model(emb_dim, vocab)\n",
        "    threat_criterion = nn.BCEWithLogitsLoss()\n",
        "    threat_optimizer = optim.Adam(threat_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    threat_valLossCount = []\n",
        "    threat_valAccCount = []\n",
        "    \n",
        "    threat_lossCount = []\n",
        "    threat_accCount = []\n",
        "\n",
        "    threat_testLossCount = []\n",
        "    threat_testAccCount = []\n",
        "\n",
        "    ######## insult ###########\n",
        "    insult_model = model(emb_dim, vocab)\n",
        "    insult_criterion = nn.BCEWithLogitsLoss()\n",
        "    insult_optimizer = optim.Adam(insult_model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    insult_valLossCount = []\n",
        "    insult_valAccCount = []\n",
        "    \n",
        "    insult_lossCount = []\n",
        "    insult_accCount = []\n",
        "\n",
        "    insult_testLossCount = []\n",
        "    insult_testAccCount = []\n",
        "\n",
        "    ######## identity_hate ###########\n",
        "    identity_hate_model = model(emb_dim, vocab)\n",
        "    identity_hate_criterion = nn.BCEWithLogitsLoss()\n",
        "    identity_hate_optimizer = optim.Adam(identity_hate_model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    identity_hate_valLossCount = []\n",
        "    identity_hate_valAccCount = []\n",
        "    \n",
        "    identity_hate_lossCount = []\n",
        "    identity_hate_accCount = []\n",
        "\n",
        "    identity_hate_testLossCount = []\n",
        "    identity_hate_testAccCount = []\n",
        "\n",
        "    ######## total ################\n",
        "    total_prediction_train_accuracy = []\n",
        "    total_prediction_val_accuracy = []\n",
        "    total_prediction_test_accuracy = []\n",
        "\n",
        "    total_prediction_train_loss = []\n",
        "    total_prediction_val_loss = []\n",
        "    total_prediction_test_loss = []\n",
        "\n",
        "    \n",
        "    \n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        print('epoch----------------------------------------------',epoch)\n",
        "        \n",
        "        ######## toxic ###########\n",
        "        toxic_correct = 0\n",
        "        toxic_total = 0\n",
        "        toxic_runningLoss = 0.0\n",
        "        toxic_numberOfBatches = 0\n",
        "\n",
        "        ######## severe_toxic ###########\n",
        "        severe_toxic_correct = 0\n",
        "        severe_toxic_total = 0\n",
        "        severe_toxic_runningLoss = 0.0\n",
        "        severe_toxic_numberOfBatches = 0        \n",
        "\n",
        "        ######## obscene ###########\n",
        "        obscene_correct = 0\n",
        "        obscene_total = 0\n",
        "        obscene_runningLoss = 0.0\n",
        "        obscene_numberOfBatches = 0        \n",
        "\n",
        "        ######## threat ###########\n",
        "        threat_correct = 0\n",
        "        threat_total = 0\n",
        "        threat_runningLoss = 0.0\n",
        "        threat_numberOfBatches = 0        \n",
        "\n",
        "        ######## insult ###########\n",
        "        insult_correct = 0\n",
        "        insult_total = 0\n",
        "        insult_runningLoss = 0.0\n",
        "        insult_numberOfBatches = 0\n",
        "\n",
        "        ######## identity_hate ###########\n",
        "        identity_hate_correct = 0\n",
        "        identity_hate_total = 0\n",
        "        identity_hate_runningLoss = 0.0\n",
        "        identity_hate_numberOfBatches = 0\n",
        "\n",
        "        ######## total ###########\n",
        "        val_total = 0\n",
        "        test_total = 0\n",
        "\n",
        "        comb_correct = 0\n",
        "        val_comb_correct = 0\n",
        "        test_comb_correct = 0\n",
        "        \n",
        "        \n",
        "        for i, batch in enumerate(train_iter, 0):\n",
        "            # print('i',i)\n",
        "            # get the set of text sentences in the batch and length of the sentence sequences\n",
        "            batch_input, batch_input_length = batch.text\n",
        "            ########### toxic #################################################\n",
        "            toxic_labels = batch_label(batch,cat[0]) \n",
        "            # print('toxic_labels',toxic_labels)\n",
        "        \n",
        "            # zero the parameter gradients\n",
        "            toxic_optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            toxic_outputs = toxic_model(batch_input, batch_input_length)\n",
        "            toxic_loss = toxic_criterion(toxic_outputs, toxic_labels)\n",
        "            toxic_loss.backward()\n",
        "            toxic_optimizer.step()\n",
        "            ########### toxic #################################################\n",
        "\n",
        "            ##### severe_toxic #############################################\n",
        "            severe_toxic_labels = batch_label(batch,cat[1]) \n",
        "        \n",
        "            # zero the parameter gradients\n",
        "            severe_toxic_optimizer.zero_grad()\n",
        "            # print('threat_labels',threat_labels)\n",
        "            # forward + backward + optimize\n",
        "            severe_toxic_outputs = severe_toxic_model(batch_input, batch_input_length)\n",
        "            # print('outputs',threat_outputs)\n",
        "            severe_toxic_loss = severe_toxic_criterion(severe_toxic_outputs, severe_toxic_labels)\n",
        "            # print('threat_outputs',threat_outputs)\n",
        "            # print('threat_loss',threat_loss)\n",
        "\n",
        "            severe_toxic_loss.backward()\n",
        "            severe_toxic_optimizer.step()\n",
        "            #### severe_toxic ####################################################\n",
        "\n",
        "            ########## obscene #############################################\n",
        "            obscene_labels = batch_label(batch,cat[2]) \n",
        "        \n",
        "            # zero the parameter gradients\n",
        "            obscene_optimizer.zero_grad()\n",
        "            # print('threat_labels',threat_labels)\n",
        "            # forward + backward + optimize\n",
        "            obscene_outputs = obscene_model(batch_input, batch_input_length)\n",
        "            # print('outputs',threat_outputs)\n",
        "            obscene_loss = obscene_criterion(obscene_outputs, obscene_labels)\n",
        "            # print('threat_outputs',threat_outputs)\n",
        "            # print('threat_loss',threat_loss)\n",
        "\n",
        "            obscene_loss.backward()\n",
        "            obscene_optimizer.step()\n",
        "            #### obscene ####################################################\n",
        "\n",
        "            ########## threat #############################################\n",
        "            threat_labels = batch_label(batch,cat[3]) \n",
        "        \n",
        "            # zero the parameter gradients\n",
        "            threat_optimizer.zero_grad()\n",
        "            # print('threat_labels',threat_labels)\n",
        "            # forward + backward + optimize\n",
        "            threat_outputs = threat_model(batch_input, batch_input_length)\n",
        "            # print('outputs',threat_outputs)\n",
        "            threat_loss = threat_criterion(threat_outputs, threat_labels)\n",
        "            # print('threat_outputs',threat_outputs)\n",
        "            # print('threat_loss',threat_loss)\n",
        "\n",
        "            threat_loss.backward()\n",
        "          #   threat_optimizer.step()\n",
        "            #### threat ####################################################\n",
        "\n",
        "             ########## insult #############################################\n",
        "            insult_labels = batch_label(batch,cat[4]) \n",
        "        \n",
        "            # zero the parameter gradients\n",
        "            insult_optimizer.zero_grad()\n",
        "            # print('threat_labels',threat_labels)\n",
        "            # forward + backward + optimize\n",
        "            insult_outputs = insult_model(batch_input, batch_input_length)\n",
        "            # print('outputs',threat_outputs)\n",
        "            insult_loss = insult_criterion(insult_outputs, insult_labels)\n",
        "            # print('threat_outputs',threat_outputs)\n",
        "            # print('threat_loss',threat_loss)\n",
        "\n",
        "            insult_loss.backward()\n",
        "            insult_optimizer.step()\n",
        "            #### threat ####################################################\n",
        "\n",
        "             ########## identity_hate #############################################\n",
        "            identity_hate_labels = batch_label(batch,cat[5]) \n",
        "        \n",
        "            # zero the parameter gradients\n",
        "            identity_hate_optimizer.zero_grad()\n",
        "            # print('threat_labels',threat_labels)\n",
        "            # forward + backward + optimize\n",
        "            identity_hate_outputs = identity_hate_model(batch_input, batch_input_length)\n",
        "            # print('outputs',threat_outputs)\n",
        "            identity_hate_loss = identity_hate_criterion(identity_hate_outputs, identity_hate_labels)\n",
        "            # print('threat_outputs',threat_outputs)\n",
        "            # print('threat_loss',threat_loss)\n",
        "\n",
        "            identity_hate_loss.backward()\n",
        "            identity_hate_optimizer.step()\n",
        "            #### threat ####################################################\n",
        "            \n",
        "            ##### evaluate training accuracy ###############################\n",
        "            with torch.no_grad():\n",
        "              \n",
        "              toxic_outputs = (toxic_outputs>0.5).float()\n",
        "              # print('changed_out',toxic_outputs)\n",
        "              toxic_correct += (toxic_outputs == toxic_labels).sum().item()\n",
        "              # print('toxic_correct',toxic_correct)\n",
        "              # print('toxic_outputs train',toxic_outputs)\n",
        "              severe_toxic_outputs = (severe_toxic_outputs>0.5).float()\n",
        "              #   # print('threat_outputs_changed',threat_outputs)\n",
        "              severe_toxic_correct += (severe_toxic_outputs == severe_toxic_labels).sum().item()\n",
        "\n",
        "              obscene_outputs = (obscene_outputs>0.5).float()\n",
        "              #   # print('threat_outputs_changed',threat_outputs)\n",
        "              obscene_correct += (obscene_outputs == obscene_labels).sum().item()              \n",
        "            \n",
        "              threat_outputs = (threat_outputs>0.5).float()\n",
        "              #   # print('threat_outputs_changed',threat_outputs)\n",
        "              threat_correct += (threat_outputs == threat_labels).sum().item()\n",
        "\n",
        "              insult_outputs = (insult_outputs>0.5).float()\n",
        "              #   # print('threat_outputs_changed',threat_outputs)\n",
        "              insult_correct += (insult_outputs == insult_labels).sum().item()\n",
        "\n",
        "              identity_hate_outputs = (identity_hate_outputs>0.5).float()\n",
        "              #   # print('threat_outputs_changed',threat_outputs)\n",
        "              identity_hate_correct += (identity_hate_outputs == identity_hate_labels).sum().item()\n",
        "\n",
        "              comb_correct += evaluate_combined_correct(toxic_outputs,toxic_labels,severe_toxic_outputs,severe_toxic_labels,\\\n",
        "                                                        obscene_outputs,obscene_labels,threat_outputs,threat_labels,insult_outputs,insult_labels,identity_hate_outputs,identity_hate_labels)           \n",
        "              \n",
        "              ##### evaluate training accuracy ###############################    \n",
        "            \n",
        "            toxic_runningLoss += toxic_loss.item()\n",
        "            # print('toxic_runningLoss',toxic_runningLoss)\n",
        "            # count the number of batches and samples\n",
        "            toxic_numberOfBatches += 1\n",
        "            toxic_total += toxic_labels.size(0)\n",
        "\n",
        "            severe_toxic_runningLoss += severe_toxic_loss.item()\n",
        "            # print('threat_runningLoss',threat_runningLoss)\n",
        "            # count the number of batches and samples\n",
        "            severe_toxic_numberOfBatches += 1\n",
        "            severe_toxic_total += severe_toxic_labels.size(0)\n",
        "\n",
        "            obscene_runningLoss += obscene_loss.item()\n",
        "            # print('threat_runningLoss',threat_runningLoss)\n",
        "            # count the number of batches and samples\n",
        "            obscene_numberOfBatches += 1\n",
        "            obscene_total += obscene_labels.size(0)\n",
        "\n",
        "            threat_runningLoss += threat_loss.item()\n",
        "            # print('threat_runningLoss',threat_runningLoss)\n",
        "            # count the number of batches and samples\n",
        "            threat_numberOfBatches += 1\n",
        "            threat_total += threat_labels.size(0)\n",
        "\n",
        "            insult_runningLoss += insult_loss.item()\n",
        "            # print('threat_runningLoss',threat_runningLoss)\n",
        "            # count the number of batches and samples\n",
        "            insult_numberOfBatches += 1\n",
        "            insult_total += insult_labels.size(0)\n",
        "\n",
        "            identity_hate_runningLoss += identity_hate_loss.item()\n",
        "            # print('threat_runningLoss',threat_runningLoss)\n",
        "            # count the number of batches and samples\n",
        "            identity_hate_numberOfBatches += 1\n",
        "            identity_hate_total += identity_hate_labels.size(0)\n",
        "\n",
        "\n",
        "        # print('threat_total',threat_total)\n",
        "        # print('toxic_total',toxic_total)\n",
        "        \n",
        "        toxic_runningLoss = toxic_runningLoss/toxic_numberOfBatches\n",
        "        toxic_correct = toxic_correct/toxic_total\n",
        "        toxic_lossCount += [toxic_runningLoss]\n",
        "        toxic_accCount += [toxic_correct]\n",
        "\n",
        "        severe_toxic_runningLoss = severe_toxic_runningLoss/severe_toxic_numberOfBatches\n",
        "        severe_toxic_correct = severe_toxic_correct/severe_toxic_total\n",
        "        severe_toxic_lossCount += [severe_toxic_runningLoss]\n",
        "        severe_toxic_accCount += [severe_toxic_correct]\n",
        "\n",
        "        obscene_runningLoss = obscene_runningLoss/obscene_numberOfBatches\n",
        "        obscene_correct = obscene_correct/obscene_total\n",
        "        obscene_lossCount += [obscene_runningLoss]\n",
        "        obscene_accCount += [obscene_correct]\n",
        "\n",
        "        threat_runningLoss = threat_runningLoss/threat_numberOfBatches\n",
        "        threat_correct = threat_correct/threat_total\n",
        "        threat_lossCount += [threat_runningLoss]\n",
        "        threat_accCount += [threat_correct]\n",
        "\n",
        "        insult_runningLoss = insult_runningLoss/insult_numberOfBatches\n",
        "        insult_correct = insult_correct/insult_total\n",
        "        insult_lossCount += [insult_runningLoss]\n",
        "        insult_accCount += [insult_correct]\n",
        "\n",
        "        identity_hate_runningLoss = identity_hate_runningLoss/identity_hate_numberOfBatches\n",
        "        identity_hate_correct = identity_hate_correct/identity_hate_total\n",
        "        identity_hate_lossCount += [identity_hate_runningLoss]\n",
        "        identity_hate_accCount += [identity_hate_correct]\n",
        "        \n",
        "        epochCount += [epoch+1]             \n",
        "        \n",
        "        avg_loss_each = (toxic_runningLoss + severe_toxic_runningLoss + obscene_runningLoss + threat_runningLoss\\\n",
        "                              + insult_runningLoss + identity_hate_runningLoss)/6\n",
        "\n",
        "        \n",
        "        total_prediction_train_loss += [avg_loss_each]\n",
        "        total_prediction_train_accuracy += [comb_correct/threat_total]\n",
        "\n",
        "        ######### validation dataset ###################################\n",
        "        print('cat',cat)\n",
        "        toxic_valRunningLoss,toxic_valCorrect,severe_toxic_valRunningLoss,severe_toxic_valCorrect,obscene_valRunningLoss,obscene_valCorrect,threat_valRunningLoss,threat_valCorrect,\\\n",
        "        insult_valRunningLoss,insult_valCorrect,identity_hate_valRunningLoss,identity_hate_valCorrect,val_comb_correct,val_total= \\\n",
        "        evaluate_binary_loss_correct(val_iter,toxic_model,severe_toxic_model,obscene_model,threat_model,insult_model,identity_hate_model,cat,learning_rate)\n",
        "        print('val_total',val_total)\n",
        "        \n",
        "        avg_val_loss_each = (toxic_valRunningLoss + severe_toxic_valRunningLoss + obscene_valRunningLoss + threat_valRunningLoss\\\n",
        "        + insult_valRunningLoss + identity_hate_valRunningLoss)/6\n",
        "\n",
        "        total_prediction_val_loss += [avg_val_loss_each]\n",
        "        total_prediction_val_accuracy += [val_comb_correct/val_total]\n",
        "\n",
        "        toxic_valLossCount += [toxic_valRunningLoss]\n",
        "        toxic_valAccCount += [toxic_valCorrect]\n",
        "\n",
        "        severe_toxic_valLossCount += [severe_toxic_valRunningLoss]\n",
        "        severe_toxic_valAccCount += [severe_toxic_valCorrect]\n",
        "\n",
        "        obscene_valLossCount += [obscene_valRunningLoss]\n",
        "        obscene_valAccCount += [obscene_valCorrect]\n",
        "\n",
        "        threat_valLossCount += [threat_valRunningLoss]\n",
        "        threat_valAccCount += [threat_valCorrect]\n",
        "\n",
        "        insult_valLossCount += [insult_valRunningLoss]\n",
        "        insult_valAccCount += [insult_valCorrect]\n",
        "        \n",
        "        identity_hate_valLossCount += [identity_hate_valRunningLoss]\n",
        "        identity_hate_valAccCount += [identity_hate_valCorrect]\n",
        "\n",
        "        ######### validation dataset ###################################\n",
        "        \n",
        "        ######### testing dataset ###################################\n",
        "        toxic_testRunningLoss,toxic_testCorrect,severe_toxic_testRunningLoss,severe_toxic_testCorrect,obscene_testRunningLoss,obscene_testCorrect,threat_testRunningLoss,threat_testCorrect,\\\n",
        "        insult_testRunningLoss,insult_testCorrect,identity_hate_testRunningLoss,identity_hate_testCorrect,test_comb_correct,test_total= \\\n",
        "        evaluate_binary_loss_correct(test_iter,toxic_model,severe_toxic_model,obscene_model,threat_model,insult_model,identity_hate_model,cat,learning_rate)\n",
        "        \n",
        "        avg_test_loss_each = (toxic_testRunningLoss + severe_toxic_testRunningLoss + obscene_testRunningLoss + threat_testRunningLoss\\\n",
        "                              + insult_testRunningLoss + identity_hate_testRunningLoss)/6\n",
        "\n",
        "        total_prediction_test_loss += [avg_test_loss_each]\n",
        "        total_prediction_test_accuracy += [test_comb_correct/test_total]\n",
        "\n",
        "        toxic_testLossCount += [toxic_testRunningLoss]\n",
        "        toxic_testAccCount += [toxic_testCorrect]\n",
        "\n",
        "        severe_toxic_testLossCount += [severe_toxic_testRunningLoss]\n",
        "        severe_toxic_testAccCount += [severe_toxic_testCorrect]\n",
        "\n",
        "        obscene_testLossCount += [obscene_testRunningLoss]\n",
        "        obscene_testAccCount += [obscene_testCorrect]\n",
        "\n",
        "        threat_testLossCount += [threat_testRunningLoss]\n",
        "        threat_testAccCount += [threat_testCorrect] \n",
        "        \n",
        "        insult_testLossCount += [insult_testRunningLoss]\n",
        "        insult_testAccCount += [insult_testCorrect]\n",
        "        \n",
        "        identity_hate_testLossCount += [identity_hate_testRunningLoss]\n",
        "        identity_hate_testAccCount += [identity_hate_testCorrect]      \n",
        "\n",
        "        print('[%d] toxic_train loss: %.3f, toxic_train acc: %.3f, toxic_validation loss: %.3f, toxic_validation acc: %.3f, \\\n",
        "        toxic_testing loss: %.3f, toxic_testing acc: %.3f ' %\n",
        "                      (epoch + 1, toxic_runningLoss, toxic_correct,toxic_valRunningLoss, toxic_valCorrect,toxic_testRunningLoss,toxic_testCorrect))\n",
        "        \n",
        "        print('[%d] severe_toxic_train loss: %.3f, severe_toxic_train acc: %.3f, severe_toxic_validation loss: %.3f, severe_toxic_validation acc: %.3f,\\\n",
        "        severe_toxic_testing loss: %.3f, severe_toxic_testing acc: %.3f ' %\n",
        "                      (epoch + 1, severe_toxic_runningLoss, severe_toxic_correct, severe_toxic_valRunningLoss, severe_toxic_valCorrect,severe_toxic_testRunningLoss,severe_toxic_testCorrect))\n",
        "        \n",
        "        print('[%d] obscene_train loss: %.3f, obscene_train acc: %.3f, obscene_validation loss: %.3f, obscene_validation acc: %.3f,\\\n",
        "        obscene_testing loss: %.3f, obscene_testing acc: %.3f ' %\n",
        "                      (epoch + 1, obscene_runningLoss,obscene_correct, obscene_valRunningLoss, obscene_valCorrect, obscene_testRunningLoss, obscene_testCorrect))\n",
        "        \n",
        "        print('[%d] threat_train loss: %.3f, threat_train acc: %.3f, threat_validation loss: %.3f, threat_validation acc: %.3f,\\\n",
        "        threat_testing loss: %.3f, threat_testing acc: %.3f ' %\n",
        "                      (epoch + 1, threat_runningLoss, threat_correct, threat_valRunningLoss, threat_valCorrect,threat_testRunningLoss,threat_testCorrect))\n",
        "        \n",
        "        print('[%d] insult_train loss: %.3f, insult_train acc: %.3f, insult_validation loss: %.3f, insult_validation acc: %.3f,\\\n",
        "        insult_testing loss: %.3f, insult_testing acc: %.3f ' %\n",
        "                      (epoch + 1, insult_runningLoss, insult_correct, insult_valRunningLoss, insult_valCorrect, insult_testRunningLoss, insult_testCorrect))\n",
        "        \n",
        "        print('[%d] identity_hate_train loss: %.3f, identity_hate_train acc: %.3f, identity_hate_validation loss: %.3f, identity_hate_validation acc: %.3f,\\\n",
        "        identity_hate_testing loss: %.3f, identity_hate_testing acc: %.3f ' %\n",
        "                      (epoch + 1, identity_hate_runningLoss, identity_hate_correct, identity_hate_valRunningLoss, identity_hate_valCorrect, identity_hate_testRunningLoss, identity_hate_testCorrect))\n",
        "        \n",
        "        print('[%d] comb_train loss: %.3f, comb_train acc: %.3f, comb_validation loss: %.3f, comb_validation acc: %.3f,\\\n",
        "        comb_testing loss: %.3f, comb_testing acc: %.3f  ' %\n",
        "                      (epoch + 1, (toxic_runningLoss+threat_runningLoss)/2, comb_correct/threat_total,\\\n",
        "                       (toxic_valRunningLoss+threat_valRunningLoss)/2,val_comb_correct/val_total,\\\n",
        "                       (toxic_testRunningLoss+threat_testRunningLoss)/2,test_comb_correct/test_total))\n",
        " \n",
        "# ------------------------------- plot --------------------------------------------\n",
        "    \n",
        "    ################# toxic ###############################\n",
        "    plt.title(\"toxic_Loss Curve\")\n",
        "    plt.plot(epochCount, toxic_lossCount, label=\"Training\")\n",
        "    plt.plot(epochCount, toxic_valLossCount, label=\"Validation\")\n",
        "    plt.plot(epochCount, toxic_testLossCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"toxic_Accuracy Curve\")\n",
        "    plt.plot(epochCount, toxic_accCount, label = \"Training\")\n",
        "    plt.plot(epochCount, toxic_valAccCount, label = \"Validation\")\n",
        "    plt.plot(epochCount, toxic_testAccCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    ################# severe_toxic ###############################\n",
        "    plt.title(\"severe_toxic_Loss Curve\")\n",
        "    plt.plot(epochCount, severe_toxic_lossCount, label=\"Training\")\n",
        "    plt.plot(epochCount, severe_toxic_valLossCount, label=\"Validation\")\n",
        "    plt.plot(epochCount, severe_toxic_testLossCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"severe_toxic_Accuracy Curve\")\n",
        "    plt.plot(epochCount, severe_toxic_accCount, label = \"Training\")\n",
        "    plt.plot(epochCount, severe_toxic_valAccCount, label = \"Validation\")\n",
        "    plt.plot(epochCount, severe_toxic_testAccCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    ################# obscene ###############################\n",
        "    plt.title(\"obscene_Loss Curve\")\n",
        "    plt.plot(epochCount, obscene_lossCount, label=\"Training\")\n",
        "    plt.plot(epochCount, obscene_valLossCount, label=\"Validation\")\n",
        "    plt.plot(epochCount, obscene_testLossCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"obscene_Accuracy Curve\")\n",
        "    plt.plot(epochCount, obscene_accCount, label = \"Training\")\n",
        "    plt.plot(epochCount, obscene_valAccCount, label = \"Validation\")\n",
        "    plt.plot(epochCount, obscene_testAccCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    ################# threat ###############################\n",
        "    plt.title(\"threat_Loss Curve\")\n",
        "    plt.plot(epochCount, threat_lossCount, label=\"Training\")\n",
        "    plt.plot(epochCount, threat_valLossCount, label=\"Validation\")\n",
        "    plt.plot(epochCount, threat_testLossCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.title(\"threat_Accuracy Curve\")\n",
        "    plt.plot(epochCount, threat_accCount, label = \"Training\")\n",
        "    plt.plot(epochCount, threat_valAccCount, label = \"Validation\")\n",
        "    plt.plot(epochCount, threat_testAccCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    ################# insult ###############################\n",
        "    plt.title(\"insult_Loss Curve\")\n",
        "    plt.plot(epochCount, insult_lossCount, label=\"Training\")\n",
        "    plt.plot(epochCount, insult_valLossCount, label=\"Validation\")\n",
        "    plt.plot(epochCount, insult_testLossCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"insult_Accuracy Curve\")\n",
        "    plt.plot(epochCount, insult_accCount, label = \"Training\")\n",
        "    plt.plot(epochCount, insult_valAccCount, label = \"Validation\")\n",
        "    plt.plot(epochCount, insult_testAccCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    ################# identity_hate ###############################\n",
        "    plt.title(\"identity_hate_Loss Curve\")\n",
        "    plt.plot(epochCount,identity_hate_lossCount, label=\"Training\")\n",
        "    plt.plot(epochCount, identity_hate_valLossCount, label=\"Validation\")\n",
        "    plt.plot(epochCount, identity_hate_testLossCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"identity_hate_Accuracy Curve\")\n",
        "    plt.plot(epochCount, identity_hate_accCount, label = \"Training\")\n",
        "    plt.plot(epochCount, identity_hate_valAccCount, label = \"Validation\")\n",
        "    plt.plot(epochCount, identity_hate_testAccCount, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    ################# total ###############################\n",
        "\n",
        "    plt.title(\"total_Loss Curve\")\n",
        "    plt.plot(epochCount, total_prediction_train_loss, label=\"Training\")\n",
        "    plt.plot(epochCount, total_prediction_val_loss, label=\"Validation\")\n",
        "    plt.plot(epochCount, total_prediction_test_loss, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"total_Accuracy Curve\")\n",
        "    plt.plot(epochCount, total_prediction_train_accuracy, label = \"Training\")\n",
        "    plt.plot(epochCount, total_prediction_val_accuracy, label = \"Validation\")\n",
        "    plt.plot(epochCount, total_prediction_test_accuracy, label=\"Testing\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    # torch.save(model,cat+'_'+'model_CNN_LSTM.pt')\n",
        "\n",
        "    # return total_prediction_train,total_prediction_val,total_prediction_test,lossCount,valLossCount,testLossCount\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}