{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IURKKDGHKf9v",
        "outputId": "c70e06e4-aae4-4cd5-ec1d-8ea9e9e038da"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU9F34eUKmAl"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import argparse\n",
        "import os\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk46w9BgKnwJ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Baseline(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, vocab):\n",
        "        super(Baseline, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)\n",
        "        self.fc = nn.Linear(embedding_dim, 6)\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        average = embedded.mean(0)\n",
        "        output = self.fc(average)\n",
        "        output = nn.functional.sigmoid(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U8apG2nLto3",
        "outputId": "adec7ac8-9403-4def-e9cb-c90aae00b557"
      },
      "source": [
        "!unzip processed_data.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  processed_data.zip\n",
            "  inflating: processed_data/valid.csv  \n",
            "  inflating: processed_data/test.csv  \n",
            "  inflating: processed_data/train.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m45yUls1M5_q"
      },
      "source": [
        "def evaluate(model, data_iter, loss_fnc):\n",
        "    sumloss = 0\n",
        "    sumcorrect = 0\n",
        "    for i, texts in enumerate(data_iter):\n",
        "        words, length = texts.comment_text\n",
        "        labels = []\n",
        "        for i in range(len(texts.toxic)):\n",
        "            labels += [[int(texts.toxic[i]), int(texts.severe_toxic[i]), int(texts.obscene[i]), int(texts.threat[i]), int(texts.insult[i]), int(texts.identity_hate[i])]]\n",
        "        out = model(words, length)\n",
        "        temploss = loss_fnc(input=out.squeeze(), target=torch.FloatTensor(labels))\n",
        "        sumloss += temploss\n",
        "        temp = ((out > 0.5).squeeze().long() == torch.FloatTensor(labels))\n",
        "        correct = 0\n",
        "        for i in temp:\n",
        "          if torch.equal(i, torch.BoolTensor([True, True, True, True, True, True])):\n",
        "            correct += 1\n",
        "    loss = torch.mean(float(sumloss) / (i + 1))\n",
        "    return float(correct)/len(data_iter.dataset), loss"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuQY9eHEM5sZ"
      },
      "source": [
        "def plot_data(train_acc, valid_acc, test_acc, train_loss, valid_loss, test_loss):\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.plot(train_acc, label=\"Training\")\n",
        "    plt.plot(valid_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.figtext(0.5, -0.05, \"Training Final Accuracy: \" + str(train_acc[-1]), wrap=True, horizontalalignment='center', fontsize=12)\n",
        "    plt.figtext(0.5, -0.1, \"Validation Final Accuracy: \" + str(valid_acc[-1]), wrap=True, horizontalalignment='center', fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Loss\")\n",
        "    plt.plot(train_loss, label=\"Training\")\n",
        "    plt.plot(valid_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye_9whlPMTzA"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def baselinetrain(batch_size, lr, epochs, emb_dim):\n",
        "    ######\n",
        "    # 3.2 Processing of the data\n",
        "    # the code below assumes you have processed and split the data into\n",
        "    # the three files, train.tsv, validation.tsv and test.tsv\n",
        "    # and those files reside in the folder named \"data\".\n",
        "    ######\n",
        "\n",
        "    COMMENT_TEXT    = data.Field(sequential=True,lower=True, tokenize='spacy', include_lengths=True)\n",
        "    TOXIC           = data.Field(sequential=False, use_vocab=False)\n",
        "    SEVERE_TOXIC    = data.Field(sequential=False, use_vocab=False)\n",
        "    OBSCENE         = data.Field(sequential=False, use_vocab=False)\n",
        "    THREAT          = data.Field(sequential=False, use_vocab=False)\n",
        "    INSULT          = data.Field(sequential=False, use_vocab=False)\n",
        "    IDENTITY_HATE   = data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "    train_data, val_data, test_data = data.TabularDataset.splits(\n",
        "            path='processed_data/', train='train.csv',\n",
        "            validation='valid.csv', test='test.csv', format='csv',\n",
        "            skip_header=True, fields=[('id', None), ('comment_text', COMMENT_TEXT), ('toxic', TOXIC), ('severe_toxic', SEVERE_TOXIC), ('obscene', OBSCENE), ('threat', THREAT), ('insult', INSULT), ('identity_hate', IDENTITY_HATE)])\n",
        "\n",
        "    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "          (train_data, val_data, test_data), batch_sizes=(batch_size, batch_size, batch_size),\n",
        "\tsort_key=lambda x: len(x.comment_text), device=None, sort_within_batch=True, repeat=False)\n",
        "\n",
        "    COMMENT_TEXT.build_vocab(train_data, val_data, test_data)\n",
        "\n",
        "    COMMENT_TEXT.vocab.load_vectors(torchtext.vocab.GloVe(name='6B', dim=100))\n",
        "    vocab = COMMENT_TEXT.vocab\n",
        "\n",
        "    print(\"Shape of Vocab:\",COMMENT_TEXT.vocab.vectors.shape)\n",
        "\n",
        "    filters = (2, 4)\n",
        "    loss_fnc = torch.nn.MSELoss()\n",
        "    model = Baseline(emb_dim, COMMENT_TEXT.vocab)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    train_acc = []\n",
        "    train_loss = []\n",
        "    valid_acc = []\n",
        "    valid_loss = []\n",
        "    test_acc = []\n",
        "    test_loss = []\n",
        "    for epoch in range(epochs):\n",
        "        for i, texts in enumerate(train_iter):\n",
        "            words, length = texts.comment_text\n",
        "            print(words)\n",
        "            labels = []\n",
        "            for i in range(len(texts.toxic)):\n",
        "              labels += [[int(texts.toxic[i]), int(texts.severe_toxic[i]), int(texts.obscene[i]), int(texts.threat[i]), int(texts.insult[i]), int(texts.identity_hate[i])]]\n",
        "            optimizer.zero_grad()\n",
        "            out = model(words, length)\n",
        "            temploss = loss_fnc(input=out.squeeze(), target=torch.FloatTensor(labels)\n",
        "            temploss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train = evaluate(model, train_iter, loss_fnc)\n",
        "        train_acc.append(train[0])\n",
        "        train_loss.append(train[1].numpy())\n",
        "        val = evaluate(model, val_iter, loss_fnc)\n",
        "        valid_acc.append(val[0])\n",
        "        valid_loss.append(val[1].numpy())\n",
        "        test = evaluate(model, test_iter, loss_fnc)\n",
        "        test_acc.append(test[0])\n",
        "        test_loss.append(test[1].numpy())\n",
        "        print(\"Epoch: {}| Train acc: {} | Train loss: {} |  Valid acc: {} |  Valid loss: {}\".format(epoch + 1, train_acc[epoch], train_loss[epoch],valid_acc[epoch], valid_loss[epoch]))\n",
        "\n",
        "    plot_data(train_acc, valid_acc, test_acc, train_loss, valid_loss, test_loss)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRueRJ9nmNMS"
      },
      "source": [
        "baselinetrain(64, 0.1, 30, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfpks5rdnNp4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "0abdb96d-309c-46f2-98d9-2681259c7648"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch\n",
        "from torch.nn import init\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, feat_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.pipeline = nn.Sequential(\n",
        "            nn.Linear(in_features=feat_size, out_features=20),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=.2),\n",
        "            nn.Linear(in_features=20, out_features=10),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(in_features=10, out_features=6)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.pipeline(x)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return probs\n",
        "\n",
        "\n",
        "def prepocess():\n",
        "    train = pd.read_csv('/content/processed_data/train.csv')\n",
        "    test = pd.read_csv('/content/processed_data/test.csv')\n",
        "    print(train.shape, test.shape)\n",
        "\n",
        "    train_rows = train.shape[0]\n",
        "\n",
        "    y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "    train.drop(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n",
        "\n",
        "    data = pd.concat([train, test])\n",
        "    del train\n",
        "    del test\n",
        "\n",
        "    # stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "    def preprocess_input(comment):\n",
        "        comment = comment.strip()\n",
        "        comment = comment.lower()\n",
        "        words = re.split('\\s', comment)\n",
        "        # words = [word for word in words if not word in stop_words]\n",
        "        sentence = ' '.join(words)\n",
        "        return sentence\n",
        "\n",
        "    data.comment_text = data.comment_text.apply(lambda row: preprocess_input(row))\n",
        "    # from nltk.tokenize import word_tokenize\n",
        "\n",
        "    \"\"\"min_df: 3--->10; result:.9724--->.9735\"\"\"\n",
        "    vect = TfidfVectorizer(min_df=10, max_df=0.7,\n",
        "                           analyzer='word',\n",
        "                           ngram_range=(1, 2),\n",
        "                           strip_accents='unicode',\n",
        "                           smooth_idf=True,\n",
        "                           sublinear_tf=True,\n",
        "                           max_features=30000\n",
        "                           )\n",
        "\n",
        "    vect = vect.fit(data['comment_text'])\n",
        "\n",
        "    # print(vect.vocabulary_)\n",
        "    # print(len(vect.vocabulary_))\n",
        "    # exit()\n",
        "\n",
        "    data_tranformed = vect.transform(data['comment_text'])\n",
        "    test = data_tranformed[train_rows:]\n",
        "    train = data_tranformed[:train_rows]\n",
        "    y_train = np.array(y_train).astype(np.float32)\n",
        "\n",
        "    return train, y_train, test\n",
        "\n",
        "    # print(\"train.shape \", train.shape)\n",
        "    #\n",
        "    #\n",
        "    # cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    # y_pred = pd.read_csv('./data/sample_submission.csv')\n",
        "    #\n",
        "    # for c in cols:\n",
        "    #     clf = LogisticRegression(C=4, solver='sag')\n",
        "    #     clf.fit(train, y_train[c])\n",
        "    #     y_pred[c] = clf.predict_proba(test)[:,1]\n",
        "    #     pred_train = clf.predict_proba(train)[:,1]\n",
        "    #     print(c, '--> log loss:', log_loss(y_train[c], pred_train))\n",
        "    #\n",
        "    #\n",
        "    # y_pred.to_csv('my_submission.csv', index=False)\n",
        "\n",
        "\n",
        "def init_param(self):\n",
        "    if isinstance(self, (nn.Conv2d, nn.Linear)):\n",
        "        init.xavier_uniform(self.weight.data)\n",
        "        init.constant(self.bias.data, 0)\n",
        "\n",
        "\n",
        "def batch_generator(batch_size, batch_x, batch_y=None, shuffle=True):\n",
        "    num_examples = batch_x.shape[0]\n",
        "    indices = list(range(num_examples))\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    counter = 0\n",
        "    mini_batch_x = []\n",
        "    mini_batch_y = []\n",
        "    for idx in indices:\n",
        "        mini_batch_x.append(batch_x[idx].toarray())\n",
        "        if batch_y is not None:\n",
        "            mini_batch_y.append(batch_y[idx])\n",
        "        counter += 1\n",
        "        if counter == batch_size:\n",
        "            counter = 0\n",
        "            xs = np.concatenate(mini_batch_x, axis=0)\n",
        "            ys = None if batch_y is None else np.stack(mini_batch_y, axis=0)\n",
        "            yield xs, ys\n",
        "            mini_batch_x = []\n",
        "            mini_batch_y = []\n",
        "\n",
        "    if len(mini_batch_x) > 0:\n",
        "        xs = np.concatenate(mini_batch_x, axis=0)\n",
        "        ys = None if batch_y is None else np.stack(mini_batch_y, axis=0)\n",
        "        yield xs, ys\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # prepare data\n",
        "    train, y_train, test = prepocess()\n",
        "\n",
        "    mlp = MLP(feat_size=100)\n",
        "    mlp.apply(init_param)\n",
        "    mlp.cuda()\n",
        "    optimizer = optim.SGD(params=mlp.parameters(), lr=3e-3, momentum=.9, weight_decay=1e-4)\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=.9)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    min_loss = 10\n",
        "    for i in range(100):\n",
        "        losses = []\n",
        "        mlp.train()\n",
        "        print(\"training....\")\n",
        "        for xs, ys in batch_generator(100, train, y_train):\n",
        "            print(xs.shape)\n",
        "            xs = Variable(torch.FloatTensor(xs).cuda())\n",
        "            ys = Variable(torch.FloatTensor(ys).cuda())\n",
        "            prob = mlp(xs)\n",
        "            loss = criterion(prob, ys)\n",
        "            losses.append(loss.data)\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        loss_ = torch.mean(torch.cat(losses))\n",
        "        print(\"epoch {}, loss {}\".format(i, loss_))\n",
        "        mlp.eval()\n",
        "\n",
        "        probs = []\n",
        "        print(\"testing...\")\n",
        "        for xs, _ in batch_generator(100, test):\n",
        "            xs = Variable(torch.FloatTensor(xs).cuda(), volatile=True)\n",
        "            prob = mlp(xs)\n",
        "            probs.append(prob.data)\n",
        "        probs = torch.cat(probs, dim=0).cpu().numpy()\n",
        "        print(probs.shape)\n",
        "        predictions = pd.read_csv(\"./data/sample_submission.csv\")\n",
        "\n",
        "        for j, c in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
        "            predictions[c] = probs[:, j]\n",
        "        if loss_ < min_loss:\n",
        "            min_loss = loss_\n",
        "            predictions.to_csv(\"submission_tf_idf_mlp_%d.csv\" % i, index=False)\n",
        "        print(\"submission saved!\")\n",
        "        print(\"******************************************\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25960, 8) (3246, 8)\n",
            "training....\n",
            "(100, 24177)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:100: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:101: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-7b5a831409ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-7b5a831409ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_MMJkB4FC2W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
